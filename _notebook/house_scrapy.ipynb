{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "]\n",
    "\n",
    "def create_headers():\n",
    "    headers = dict()\n",
    "    headers[\"User-Agent\"] = random.choice(USER_AGENTS)\n",
    "    headers[\"Referer\"] = \"http://www.ke.com\"\n",
    "    return headers\n",
    "\n",
    "# 请求网页数据函数\n",
    "def get_html(url):        \n",
    "    headers = create_headers()\n",
    "    response = requests.get(url, timeout=10, headers=headers)\n",
    "    html = response.content\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# 开始获得需要的板块数据\n",
    "total_page = 1\n",
    "loupan_list = list()\n",
    "url = 'https://cq.fang.ke.com/loupan/'\n",
    "\n",
    "soup = BeautifulSoup(get_html(url), \"lxml\")\n",
    "# 获得总的页数\n",
    "page_box = soup.find_all('div', class_='page-box')[0]\n",
    "matches = re.search('.*data-total-count=\"(\\d+)\".*', str(page_box))\n",
    "total_page = int(math.ceil(int(matches.group(1)) / 10))\n",
    "print(total_page, url)\n",
    "\n",
    "\n",
    "# 从第一页开始,一直遍历到最后一页\n",
    "headers = create_headers()\n",
    "for i in range(1, total_page + 1):\n",
    "    page = url + '/pg{0}'.format(i)\n",
    "    print(page)\n",
    "    soup = BeautifulSoup(get_html(page), \"lxml\")\n",
    "\n",
    "    # 获得有小区信息的panel\n",
    "    house_elements = soup.find_all('li', class_=\"resblock-list\")\n",
    "    for house_elem in house_elements:\n",
    "        price = house_elem.find('span', class_=\"number\")\n",
    "        desc = house_elem.find('span', class_=\"desc\")\n",
    "        total = house_elem.find('div', class_=\"second\")\n",
    "        loupan = house_elem.find('a', class_='name')\n",
    "\n",
    "\n",
    "        # 继续清理数据\n",
    "        try:\n",
    "            price = price.text.strip() + desc.text.strip()\n",
    "        except Exception as e:\n",
    "            price = '0'\n",
    "        loupan = loupan.text.replace(\"\\n\", \"\")\n",
    "\n",
    "        try:\n",
    "            total = total.text.strip().replace(u'总价', '')\n",
    "            total = total.replace(u'/套起', '')\n",
    "        except Exception as e:\n",
    "            total = '0'\n",
    "        print(\"price:{0}, desc:{1}, total:{2}, loupan:{3}\".format(price, desc, total, loupan))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "chinese_city_district_dict = dict()\n",
    "chinese_area_dict = dict()\n",
    "\n",
    "base_site = 'https://cq.ke.com/'\n",
    "\n",
    "\n",
    "def get_districts():\n",
    "    url = base_site + 'xiaoqu/'\n",
    "    \n",
    "    root = etree.HTML(get_html(url))\n",
    "    elements = root.xpath('///div[3]/div[1]/dl[2]/dd/div/div/a')\n",
    "    en_names = list()\n",
    "    ch_names = list()\n",
    "    for element in elements:\n",
    "        link = element.attrib['href']\n",
    "        en_names.append(link.split('/')[-2])\n",
    "        ch_names.append(element.text)\n",
    "\n",
    "\n",
    "    # 打印区县英文和中文名列表\n",
    "    for index, name in enumerate(en_names):\n",
    "        chinese_city_district_dict[name] = ch_names[index]\n",
    "    return en_names\n",
    "\n",
    "\n",
    "def get_areas(district):\n",
    "    page = base_site + \"xiaoqu/{0}\".format(district)\n",
    "    areas = list()\n",
    "    try:\n",
    "        root = etree.HTML(get_html(page))\n",
    "        links = root.xpath('//div[3]/div[1]/dl[2]/dd/div/div[2]/a')\n",
    "\n",
    "        # 针对a标签的list进行处理\n",
    "        for link in links:\n",
    "            relative_link = link.attrib['href']\n",
    "            # 去掉最后的\"/\"\n",
    "            relative_link = relative_link[:-1]\n",
    "            # 获取最后一节\n",
    "            area = relative_link.split(\"/\")[-1]\n",
    "            # 去掉区县名,防止重复\n",
    "            if area != district:\n",
    "                chinese_area = link.text\n",
    "                chinese_area_dict[area] = chinese_area\n",
    "                # print(chinese_area)\n",
    "                areas.append(area)\n",
    "        return areas\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "with open(\"sechouse.csv\", \"a+\", encoding='utf-8') as f:\n",
    "    # 开始获得需要的板块数据\n",
    "    total_page = 1\n",
    "    sec_house_list = list()\n",
    "    districts = get_districts()\n",
    "    for district in districts:\n",
    "        arealist = get_areas(district)\n",
    "        for area in arealist:\n",
    "            # 中文区县\n",
    "            chinese_district = chinese_city_district_dict.get(district, \"\")\n",
    "            # 中文版块\n",
    "            chinese_area = chinese_area_dict.get(area, \"\")\n",
    "            page = base_site + 'ershoufang/{0}/'.format(area)\n",
    "            if area in ['beibinlu', 'dashiba' , 'dazhulin', 'guanyinqiao','haierlu', 'hongensi','huahuiyuan','huangnibang','jiangbeizui','jiazhou','longtousi']:\n",
    "                print(\"skip: \", chinese_area)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(get_html(page), \"lxml\")\n",
    "            # 获得总的页数\n",
    "            page_box = soup.find_all('div', class_='house-lst-page-box')[0]\n",
    "            matches = re.search('.*\"totalPage\":(\\d+).*', str(page_box))\n",
    "            total_page = int(matches.group(1))\n",
    "            print(chinese_district, chinese_area, total_page, page)\n",
    "            \n",
    "            # 从第一页开始,一直遍历到最后一页\n",
    "            for i in range(1, total_page + 1):\n",
    "                page = base_site + 'ershoufang/{0}/pg{1}'.format(area, i)\n",
    "\n",
    "                if area == 'longxing' and i < 51:\n",
    "                    print('skip:', page)\n",
    "\n",
    "                soup = BeautifulSoup(get_html(page), \"lxml\")\n",
    "                # 获得有小区信息的panel\n",
    "                house_elements = soup.find_all('li', class_=\"clear\")\n",
    "                for house_elem in house_elements:\n",
    "                    pic = house_elem.find('a', class_=\"img\").find('img', class_=\"lj-lazy\").get('data-original').strip()\n",
    "                    name = house_elem.find('div', class_='title').text.replace(\"\\n\", \"\").replace(',',';').strip()\n",
    "                    url =  house_elem.find('div', class_='title').find('a').get('href').replace(\"\\n\", \"\").strip()\n",
    "                    position = house_elem.find('div', class_='positionInfo').text.replace(\"\\n\", \"\").strip()\n",
    "                    desc = house_elem.find('div', class_=\"houseInfo\").text.replace(\"\\n\", \"\").strip()\n",
    "                    starInfo = house_elem.find('div', class_=\"followInfo\").text.replace(\"\\n\", \"\").strip()\n",
    "                    tag = house_elem.find('div', class_=\"tag\").text.replace(\"\\n\", \"\").strip()\n",
    "                    price = house_elem.find('div', class_=\"totalPrice\").text.replace(\"\\n\", \"\").strip()\n",
    "                    unit_price = house_elem.find('div', class_=\"unitPrice\").text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                    item_text = chinese_district + ', ' + chinese_area + ', ' + position + ', ' + \\\n",
    "                        name + ', ' + \\\n",
    "                        desc + ', ' + \\\n",
    "                        price + ', ' + \\\n",
    "                        unit_price + ', ' + \\\n",
    "                        starInfo + ', ' + \\\n",
    "                        tag + ', ' + \\\n",
    "                        url + ', ' + \\\n",
    "                        pic\n",
    "                    f.write(item_text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东原D7一期, 户型方正 中间楼层 朝中庭 出门轻轨和商城新上, 中楼层                        (共33层)                                                            | 2011年建 |                    3室2厅 | 93平米                        | 北, 150万, 16,130元/平, 0人关注    / 6天前发布, 近地铁 VR看装修新上, https://cq.ke.com/ershoufang/106114871021.html, https://ke-image.ljcdn.com/110000-inspection/b2e5de34-6456-4789-bf85-72675c7b66e8_1000.jpg!m_fill,w_280,h_210,f_jpg?from=ke.com\n"
     ]
    }
   ],
   "source": [
    "html_str = '''<li class=\"clear\">\n",
    "</div></li>\n",
    "'''\n",
    "\n",
    "house_elem = BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "pic = house_elem.find('a', class_=\"img\").find('img', class_=\"lj-lazy\").get('data-original').strip()\n",
    "name = house_elem.find('div', class_='title').text.replace(\"\\n\", \"\").replace(',',';').strip()\n",
    "url =  house_elem.find('div', class_='title').find('a').get('href').replace(\"\\n\", \"\").strip()\n",
    "position = house_elem.find('div', class_='positionInfo').text.replace(\"\\n\", \"\").strip()\n",
    "desc = house_elem.find('div', class_=\"houseInfo\").text.replace(\"\\n\", \"\").strip()\n",
    "starInfo = house_elem.find('div', class_=\"followInfo\").text.replace(\"\\n\", \"\").strip()\n",
    "tag = house_elem.find('div', class_=\"tag\").text.replace(\"\\n\", \"\").strip()\n",
    "price = house_elem.find('div', class_=\"totalPrice\").text.replace(\"\\n\", \"\").strip()\n",
    "unit_price = house_elem.find('div', class_=\"unitPrice\").text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "text = position + ', ' + \\\n",
    "      name + ', ' + \\\n",
    "      desc + ', ' + \\\n",
    "      price + ', ' + \\\n",
    "      unit_price + ', ' + \\\n",
    "      starInfo + ', ' + \\\n",
    "      tag + ', ' + \\\n",
    "      url + ', ' + \\\n",
    "      pic\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_loc():\n",
    "    api_url = 'https://restapi.amap.com/v3/geocode/geo'\n",
    "    \n",
    "    # 写进自己的key, 由于请求条数较多, 可能超配额了, 所以用了两个key\n",
    "    # key_list = ['你的第一个key', '你的第二个key']\n",
    "    \n",
    "    # 读取爬取下来的房价数据\n",
    "    taz_df = pd.read_csv(r'../df_ershoufang.csv')\n",
    "    \n",
    "    lat_list = []\n",
    "    lng_list = []\n",
    "    for _, row in taz_df.iterrows():\n",
    "        address_name = '陕西省西安市' + row['district'] + row['name']\n",
    "        _i = np.random.randint(0, 2)\n",
    "        para_dict = {'address': address_name, 'key': key_list[_i], 'output': 'JSON'}\n",
    "\n",
    "        try:\n",
    "            r = requests.get(api_url, params=para_dict)\n",
    "            json_data = json.loads(r.text)\n",
    "            if json_data['status'] == '1':\n",
    "                geo = json_data['geocodes'][0]['location']\n",
    "                longitude = geo.split(',')[0]\n",
    "                latitude = geo.split(',')[1]\n",
    "            else:\n",
    "                longitude = np.nan\n",
    "                latitude = np.nan\n",
    "        except:\n",
    "            longitude = np.nan\n",
    "            latitude = np.nan\n",
    "            continue\n",
    "        lat_list.append(latitude)\n",
    "        lng_list.append(longitude)\n",
    "        print(_i)\n",
    "        print(longitude, latitude)\n",
    "\n",
    "    loc_df = pd.DataFrame({'lng': lng_list, 'lat': lat_list})\n",
    "    taz_df = pd.concat([taz_df, loc_df], axis=1)\n",
    "\n",
    "    taz_df.to_csv(r'house_price.csv', encoding='utf_8_sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
